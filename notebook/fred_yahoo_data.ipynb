{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fredapi as fred\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Set the API key\n",
    "\n",
    "FRED_API_KEY = os.getenv('FRED_API_KEY')\n",
    "\n",
    "# Create a FRED API object\n",
    "fred_api = fred.Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "# Get the release dates for all releases\n",
    "release_dates = fred_api.get_series_all_releases('UNRATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 현재 작업 디렉토리 설정\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 데이터 파일 경로 설정\n",
    "data_path = os.path.join('../data', 'nowcast-model-variables.xlsx')\n",
    "\n",
    "variables = pd.read_excel(data_path, engine='openpyxl', sheet_name='variables')\n",
    "releases = pd.read_excel(data_path, engine='openpyxl', sheet_name='releases')\n",
    "\n",
    "variables.to_csv('../data/nowcast-variables.csv')\n",
    "releases.to_csv('../data/nowcast-releases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = FRED_API_KEY\n",
    "BASE_URL = 'https://api.stlouisfed.org/fred'\n",
    "\n",
    "source_keys = pd.read_csv('../data/nowcast-variables.csv')\n",
    "\n",
    "# hist_source가 fred인 경우\n",
    "fred_keys = source_keys[source_keys['hist_source'] == 'fred']['hist_source_key']\n",
    "\n",
    "# hist_source가 yahoo인 경우\n",
    "yf_tickers = source_keys[source_keys['hist_source'] == 'yahoo']['hist_source_key']\n",
    "\n",
    "# yf_tickers의 종가를 yfinacne로 다운로드하여 병합\n",
    "import yfinance as yf\n",
    "\n",
    "data_dict = {}\n",
    "for ticker in yf_tickers:\n",
    "    data = yf.download(ticker, start='2000-01-01', end='2024-12-31')\n",
    "    data = data['Close']\n",
    "    data.name = ticker\n",
    "    data_dict[ticker] = data\n",
    "\n",
    "market_df = pd.concat(data_dict.values(), axis=1)\n",
    "market_df.columns = data_dict.keys()\n",
    "market_df.index.name = 'Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PCEC96...\n",
      "Processing A068RC1...\n",
      "Processing PMSAVE...\n",
      "Processing PSAVERT...\n",
      "Processing RPI...\n",
      "Processing DSPIC96...\n",
      "Processing TOTCI...\n",
      "Processing RELACBW027SBOG...\n",
      "Processing DRSFRMACBS...\n",
      "Processing DRCCLACBS...\n",
      "Processing DRBLACBS...\n",
      "Processing GDPC1...\n",
      "Processing PCECC96...\n",
      "Processing DGDSRX1Q020SBEA...\n",
      "Processing PCDGCC96...\n",
      "Processing DMOTRX1Q020SBEA...\n",
      "Processing DFDHRX1Q020SBEA...\n",
      "Processing DREQRX1Q020SBEA...\n",
      "Processing DODGRX1Q020SBEA...\n",
      "Processing PCNDGC96...\n",
      "Processing DFXARX1Q020SBEA...\n",
      "Processing DCLORX1Q020SBEA...\n",
      "Processing DGOERX1Q020SBEA...\n",
      "Processing DONGRX1Q020SBEA...\n",
      "Processing PCESVC96...\n",
      "Processing DHUTRX1Q020SBEA...\n",
      "Processing DHLCRX1Q020SBEA...\n",
      "Processing DTRSRX1Q020SBEA...\n",
      "Processing DRCARX1Q020SBEA...\n",
      "Processing DFSARX1Q020SBEA...\n",
      "Processing DIFSRX1Q020SBEA...\n",
      "Processing DOTSRX1Q020SBEA...\n",
      "Processing DNPIRX1Q020SBEA...\n",
      "Processing GPDIC1...\n",
      "Processing PNFIC1...\n",
      "Processing B009RX1Q020SBEA...\n",
      "Processing Y033RX1Q020SBEA...\n",
      "Processing Y001RX1Q020SBEA...\n",
      "Processing PRFIC1...\n",
      "Processing CBIC1...\n",
      "Processing NETEXC...\n",
      "Processing EXPGSC1...\n",
      "Processing A253RX1Q020SBEA...\n",
      "Processing A646RX1Q020SBEA...\n",
      "Processing IMPGSC1...\n",
      "Processing A255RX1Q020SBEA...\n",
      "Processing B656RX1Q020SBEA...\n",
      "Processing GCEC1...\n",
      "Processing FGCEC1...\n",
      "Processing SLCEC1...\n",
      "Processing CSUSHPISA...\n",
      "Processing SPCS20RSA...\n",
      "Processing HOUST...\n",
      "Processing HSN1F...\n",
      "Processing PERMIT...\n",
      "Processing USAUCSFRCONDOSMSAMID...\n",
      "skip USAUCSFRCONDOSMSAMID: No metadata available\n",
      "Processing PCEPI...\n",
      "skip PCEPI: No metadata available\n",
      "Processing CPIAUCSL...\n",
      "skip CPIAUCSL: No metadata available\n",
      "Processing PPIACO...\n",
      "skip PPIACO: No metadata available\n",
      "Processing UNRATE...\n",
      "skip UNRATE: No metadata available\n",
      "Processing PAYEMS...\n",
      "skip PAYEMS: No metadata available\n",
      "Processing JTSJOL...\n",
      "skip JTSJOL: No metadata available\n",
      "Processing LMJVTTUVUSM647S...\n",
      "skip LMJVTTUVUSM647S: No metadata available\n",
      "Processing CES0500000003...\n",
      "skip CES0500000003: No metadata available\n",
      "Processing STLFSI3...\n",
      "skip STLFSI3: No metadata available\n",
      "Processing FRBKCLMCILA...\n",
      "skip FRBKCLMCILA: No metadata available\n",
      "Processing CIVPART...\n",
      "skip CIVPART: No metadata available\n",
      "Processing INDPRO...\n",
      "skip INDPRO: No metadata available\n",
      "Processing IPFPNSS...\n",
      "skip IPFPNSS: No metadata available\n",
      "Processing IPCONGD...\n",
      "skip IPCONGD: No metadata available\n",
      "Processing IPFUELS...\n",
      "skip IPFUELS: No metadata available\n",
      "Processing DGORDER...\n",
      "skip DGORDER: No metadata available\n",
      "Processing ACOGNO...\n",
      "skip ACOGNO: No metadata available\n",
      "Processing TTLCONS...\n",
      "skip TTLCONS: No metadata available\n",
      "Processing ISRATIO...\n",
      "skip ISRATIO: No metadata available\n",
      "Processing TCU...\n",
      "skip TCU: No metadata available\n",
      "Processing EFFR...\n",
      "skip EFFR: No metadata available\n",
      "Processing MORTGAGE30US...\n",
      "skip MORTGAGE30US: No metadata available\n",
      "Processing DAAA...\n",
      "skip DAAA: No metadata available\n",
      "Processing DBAA...\n",
      "skip DBAA: No metadata available\n",
      "Processing TOTALSA...\n",
      "skip TOTALSA: No metadata available\n",
      "Processing RSAFS...\n",
      "skip RSAFS: No metadata available\n",
      "Processing CSCICP03USM665S...\n",
      "skip CSCICP03USM665S: No metadata available\n",
      "Processing CEFDISA066MSFRBNY...\n",
      "skip CEFDISA066MSFRBNY: No metadata available\n",
      "Processing BACTSAMFRBDAL...\n",
      "skip BACTSAMFRBDAL: No metadata available\n",
      "Processing CEFDFSA066MSFRBPHI...\n",
      "skip CEFDFSA066MSFRBPHI: No metadata available\n",
      "Processing CBBTCUSD...\n",
      "skip CBBTCUSD: No metadata available\n",
      "Processing DCOILWTICO...\n",
      "skip DCOILWTICO: No metadata available\n",
      "메타데이터 df shape: (55, 15)\n",
      "merged_df shape: (28339, 55)\n",
      "        id realtime_start realtime_end  \\\n",
      "0   PCEC96     2025-03-19   2025-03-19   \n",
      "1  A068RC1     2025-03-19   2025-03-19   \n",
      "2   PMSAVE     2025-03-17   2025-03-17   \n",
      "3  PSAVERT     2025-03-18   2025-03-18   \n",
      "4      RPI     2025-03-19   2025-03-19   \n",
      "\n",
      "                                    title observation_start observation_end  \\\n",
      "0  Real Personal Consumption Expenditures        2007-01-01      2025-01-01   \n",
      "1                        Personal outlays        1959-01-01      2025-01-01   \n",
      "2                         Personal Saving        1959-01-01      2025-01-01   \n",
      "3                    Personal Saving Rate        1959-01-01      2025-01-01   \n",
      "4                    Real Personal Income        1959-01-01      2025-01-01   \n",
      "\n",
      "  frequency frequency_short                             units  \\\n",
      "0   Monthly               M  Billions of Chained 2017 Dollars   \n",
      "1   Monthly               M               Billions of Dollars   \n",
      "2   Monthly               M               Billions of Dollars   \n",
      "3   Monthly               M                           Percent   \n",
      "4   Monthly               M  Billions of Chained 2017 Dollars   \n",
      "\n",
      "           units_short              seasonal_adjustment  \\\n",
      "0  Bil. of Chn. 2017 $  Seasonally Adjusted Annual Rate   \n",
      "1            Bil. of $  Seasonally Adjusted Annual Rate   \n",
      "2            Bil. of $  Seasonally Adjusted Annual Rate   \n",
      "3                    %  Seasonally Adjusted Annual Rate   \n",
      "4  Bil. of Chn. 2017 $  Seasonally Adjusted Annual Rate   \n",
      "\n",
      "  seasonal_adjustment_short            last_updated  popularity  \\\n",
      "0                      SAAR  2025-02-28 07:44:25-06          74   \n",
      "1                      SAAR  2025-02-28 07:45:12-06          18   \n",
      "2                      SAAR  2025-02-28 07:45:27-06          66   \n",
      "3                      SAAR  2025-02-28 07:45:28-06          84   \n",
      "4                      SAAR  2025-02-28 07:44:30-06          58   \n",
      "\n",
      "                                               notes  \n",
      "0  BEA Account Code: DPCERX\\nA Guide to the Natio...  \n",
      "1  BEA Account Code: A068RC\\nA Guide to the Natio...  \n",
      "2  BEA Account Code: A071RC\\nA Guide to the Natio...  \n",
      "3  BEA Account Code: A072RC\\nPersonal saving as a...  \n",
      "4  Calculated by the Federal Reserve Bank of St. ...  \n",
      "             PCEC96 A068RC1 PMSAVE PSAVERT  RPI DSPIC96 TOTCI RELACBW027SBOG  \\\n",
      "release_date                                                                   \n",
      "1947-08-17      NaN     NaN    NaN     NaN  NaN     NaN   NaN            NaN   \n",
      "1947-08-18      NaN     NaN    NaN     NaN  NaN     NaN   NaN            NaN   \n",
      "1947-08-19      NaN     NaN    NaN     NaN  NaN     NaN   NaN            NaN   \n",
      "1947-08-20      NaN     NaN    NaN     NaN  NaN     NaN   NaN            NaN   \n",
      "1947-08-21      NaN     NaN    NaN     NaN  NaN     NaN   NaN            NaN   \n",
      "\n",
      "             DRSFRMACBS DRCCLACBS  ... A255RX1Q020SBEA B656RX1Q020SBEA  \\\n",
      "release_date                       ...                                   \n",
      "1947-08-17          NaN       NaN  ...             NaN             NaN   \n",
      "1947-08-18          NaN       NaN  ...             NaN             NaN   \n",
      "1947-08-19          NaN       NaN  ...             NaN             NaN   \n",
      "1947-08-20          NaN       NaN  ...             NaN             NaN   \n",
      "1947-08-21          NaN       NaN  ...             NaN             NaN   \n",
      "\n",
      "                GCEC1 FGCEC1 SLCEC1 CSUSHPISA SPCS20RSA HOUST HSN1F PERMIT  \n",
      "release_date                                                                \n",
      "1947-08-17    560.034      .      .       NaN       NaN   NaN   NaN    NaN  \n",
      "1947-08-18        NaN    NaN    NaN       NaN       NaN   NaN   NaN    NaN  \n",
      "1947-08-19        NaN    NaN    NaN       NaN       NaN   NaN   NaN    NaN  \n",
      "1947-08-20        NaN    NaN    NaN       NaN       NaN   NaN   NaN    NaN  \n",
      "1947-08-21        NaN    NaN    NaN       NaN       NaN   NaN   NaN    NaN  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "\n",
      "Skipped keys:\n",
      "USAUCSFRCONDOSMSAMID: No metadata\n",
      "PCEPI: No metadata\n",
      "CPIAUCSL: No metadata\n",
      "PPIACO: No metadata\n",
      "UNRATE: No metadata\n",
      "PAYEMS: No metadata\n",
      "JTSJOL: No metadata\n",
      "LMJVTTUVUSM647S: No metadata\n",
      "CES0500000003: No metadata\n",
      "STLFSI3: No metadata\n",
      "FRBKCLMCILA: No metadata\n",
      "CIVPART: No metadata\n",
      "INDPRO: No metadata\n",
      "IPFPNSS: No metadata\n",
      "IPCONGD: No metadata\n",
      "IPFUELS: No metadata\n",
      "DGORDER: No metadata\n",
      "ACOGNO: No metadata\n",
      "TTLCONS: No metadata\n",
      "ISRATIO: No metadata\n",
      "TCU: No metadata\n",
      "EFFR: No metadata\n",
      "MORTGAGE30US: No metadata\n",
      "DAAA: No metadata\n",
      "DBAA: No metadata\n",
      "TOTALSA: No metadata\n",
      "RSAFS: No metadata\n",
      "CSCICP03USM665S: No metadata\n",
      "CEFDISA066MSFRBNY: No metadata\n",
      "BACTSAMFRBDAL: No metadata\n",
      "CEFDFSA066MSFRBPHI: No metadata\n",
      "CBBTCUSD: No metadata\n",
      "DCOILWTICO: No metadata\n"
     ]
    }
   ],
   "source": [
    "meta_dict = {}\n",
    "data_dict = {}\n",
    "skipped_keys = []\n",
    "\n",
    "for source_key in fred_keys:\n",
    "    print(f\"Processing {source_key}...\")\n",
    "    try:\n",
    "        # 메타데이터 수집\n",
    "        meta_url = f'{BASE_URL}/series'\n",
    "        meta_params = {'api_key': API_KEY, 'file_type': 'json', 'series_id': source_key}\n",
    "        meta_r = requests.get(meta_url, params=meta_params).json().get('seriess', [])\n",
    "        if not meta_r:\n",
    "            print(f\"skip {source_key}: No metadata available\")\n",
    "            skipped_keys.append((source_key, \"No metadata\"))\n",
    "            continue\n",
    "        meta_df = pd.DataFrame(meta_r)\n",
    "        meta_dict[source_key] = meta_df\n",
    "\n",
    "        # release id\n",
    "        release_url = f'{BASE_URL}/series/release'\n",
    "        release_params = {'api_key': API_KEY, 'file_type': 'json', 'series_id': source_key}\n",
    "        release_response = requests.get(release_url, params=release_params).json()\n",
    "        if 'releases' not in release_response or not release_response['releases']:\n",
    "            print(f\"skip {source_key}: No release info\")\n",
    "            skipped_keys.append((source_key, \"No release info\"))\n",
    "            continue\n",
    "        release_id = release_response['releases'][0]['id']\n",
    "\n",
    "        # obs\n",
    "        obs_url = f'{BASE_URL}/series/observations'\n",
    "        obs_params = {'api_key': API_KEY, 'file_type': 'json', 'series_id': source_key}\n",
    "        obs_response = requests.get(obs_url, params=obs_params).json()\n",
    "        if 'observations' not in obs_response:\n",
    "            print(f\"skip {source_key}: No observations\")\n",
    "            skipped_keys.append((source_key, \"No observations\"))\n",
    "            continue\n",
    "        obs_data = obs_response['observations']\n",
    "        if not obs_data:\n",
    "            print(f\"skip {source_key}: Empty observations\")\n",
    "            skipped_keys.append((source_key, \"Empty observations\"))\n",
    "            continue\n",
    "\n",
    "        # release dates\n",
    "        rel_url = f'{BASE_URL}/release/dates'\n",
    "        rel_params = {'api_key': API_KEY, 'file_type': 'json', 'release_id': release_id}\n",
    "        rel_data = requests.get(rel_url, params=rel_params).json()['release_dates']\n",
    "        if not rel_data:\n",
    "            print(f\"skip {source_key}: No release dates\")\n",
    "            skipped_keys.append((source_key, \"No release dates\"))\n",
    "            continue\n",
    "\n",
    "        # obs_df\n",
    "        obs_df = pd.DataFrame(obs_data)\n",
    "        obs_df['date'] = pd.to_datetime(obs_df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "        obs_df['base'] = (obs_df['date'] + pd.DateOffset(months=1)).dt.to_period('M').astype(str)\n",
    "        obs_df[source_key] = obs_df['value']\n",
    "        obs_df = obs_df[['base', source_key]]\n",
    "\n",
    "        # release_df\n",
    "        rel_df = pd.DataFrame(rel_data)\n",
    "        rel_df['release_date'] = pd.to_datetime(rel_df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "        rel_df['base'] = rel_df['release_date'].dt.to_period('M').astype(str)\n",
    "        rel_df = rel_df[['release_date', 'base']]\n",
    "\n",
    "        # merge + 중복 제거 + 리샘플링\n",
    "        temp_df = obs_df.merge(rel_df, on='base', how='left')  # inner 대신 left로 변경\n",
    "        temp_df = temp_df.drop(columns=['base']).set_index('release_date')\n",
    "        temp_df = temp_df.groupby('release_date').first()  # 중복 제거\n",
    "        temp_df = temp_df.resample('D').asfreq()\n",
    "\n",
    "        data_dict[source_key] = temp_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"skip {source_key}: {e}\")\n",
    "        skipped_keys.append((source_key, str(e)))\n",
    "        continue\n",
    "\n",
    "# 데이터 병합\n",
    "merged_df = pd.concat(data_dict.values(), axis=1, join='outer')\n",
    "meta_full_df = pd.concat(meta_dict.values(), axis=0, ignore_index=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"메타데이터 df shape:\", meta_full_df.shape)\n",
    "print(\"merged_df shape:\", merged_df.shape)\n",
    "print(meta_full_df.head())\n",
    "print(merged_df.head())\n",
    "print(\"\\nSkipped keys:\")\n",
    "for key, reason in skipped_keys:\n",
    "    print(f\"{key}: {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "{'realtime_start': '2025-03-19', 'realtime_end': '2025-03-19', 'seriess': [{'id': 'DCOILWTICO', 'realtime_start': '2025-03-19', 'realtime_end': '2025-03-19', 'title': 'Crude Oil Prices: West Texas Intermediate (WTI) - Cushing, Oklahoma', 'observation_start': '1986-01-02', 'observation_end': '2025-03-10', 'frequency': 'Daily', 'frequency_short': 'D', 'units': 'Dollars per Barrel', 'units_short': '$ per Barrel', 'seasonal_adjustment': 'Not Seasonally Adjusted', 'seasonal_adjustment_short': 'NSA', 'last_updated': '2025-03-12 12:10:03-05', 'popularity': 77, 'notes': 'Definitions, Sources and Explanatory Notes (http://www.eia.doe.gov/dnav/pet/TblDefs/pet_pri_spt_tbldef2.asp)'}]}\n"
     ]
    }
   ],
   "source": [
    "source_key = \"DCOILWTICO\"\n",
    "meta_url = f'{BASE_URL}/series'\n",
    "meta_params = {'api_key': API_KEY, 'file_type': 'json', 'series_id': source_key}\n",
    "response = requests.get(meta_url, params=meta_params)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import yfinance as yf\n",
    "\n",
    "API_KEY = FRED_API_KEY\n",
    "BASE_URL = 'https://api.stlouisfed.org/fred'\n",
    "\n",
    "source_keys = pd.read_csv('../data/nowcast-variables.csv')\n",
    "fred_keys = source_keys[source_keys['hist_source'] == 'fred']['hist_source_key']\n",
    "yf_tickers = source_keys[source_keys['hist_source'] == 'yahoo']['hist_source_key']\n",
    "\n",
    "def fetch_yahoo_data(tickers, start='2000-01-01', end='2024-12-31'):\n",
    "    data_dict = {}\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching Yahoo data for {ticker}...\")\n",
    "        data = yf.download(ticker, start=start, end=end)\n",
    "        data = data['Close']\n",
    "        data.name = ticker\n",
    "        data_dict[ticker] = data\n",
    "    market_df = pd.concat(data_dict.values(), axis=1)\n",
    "    market_df.columns = data_dict.keys()\n",
    "    market_df.index.name = 'Date'\n",
    "    return market_df\n",
    "\n",
    "def fetch_fred_series(source_key, api_key, base_url, timeout=10):\n",
    "    try:\n",
    "        # 메타데이터\n",
    "        meta_url = f'{base_url}/series'\n",
    "        meta_params = {'api_key': api_key, 'file_type': 'json', 'series_id': source_key}\n",
    "        meta_response = requests.get(meta_url, params=meta_params, timeout=timeout)\n",
    "        meta_json = meta_response.json()\n",
    "        meta_r = meta_json.get('seriess', [])\n",
    "        if not meta_r:\n",
    "            return None, None, f\"No metadata available - Response: {meta_json}\"\n",
    "\n",
    "        meta_df = pd.DataFrame(meta_r)\n",
    "        frequency = meta_df['frequency_short'].iloc[0]  # 주기: D, W, M, Q 등\n",
    "\n",
    "        release_url = f'{base_url}/series/release'\n",
    "        release_params = {'api_key': api_key, 'file_type': 'json', 'series_id': source_key}\n",
    "        release_response = requests.get(release_url, params=release_params, timeout=timeout)\n",
    "        release_json = release_response.json()\n",
    "        if 'releases' not in release_json or not release_json['releases']:\n",
    "            return None, None, \"No release info\"\n",
    "        release_id = release_json['releases'][0]['id']\n",
    "\n",
    "        obs_url = f'{base_url}/series/observations'\n",
    "        obs_params = {'api_key': api_key, 'file_type': 'json', 'series_id': source_key}\n",
    "        obs_response = requests.get(obs_url, params=obs_params, timeout=timeout)\n",
    "        obs_json = obs_response.json()\n",
    "        if 'observations' not in obs_json or not obs_json['observations']:\n",
    "            return None, None, \"No observations or empty\"\n",
    "\n",
    "        obs_df = pd.DataFrame(obs_json['observations'])\n",
    "        obs_df['date'] = pd.to_datetime(obs_df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "        obs_df[source_key] = obs_df['value']\n",
    "\n",
    "        # 주기에 따라 날짜 조정\n",
    "        if frequency == 'M':  # 월간\n",
    "            obs_df['adjusted_date'] = obs_df['date'] + pd.offsets.MonthEnd(0) + pd.offsets.MonthBegin(1)\n",
    "        elif frequency == 'Q':  # 분기\n",
    "            obs_df['adjusted_date'] = obs_df['date'] + pd.offsets.QuarterEnd(0) + pd.offsets.MonthBegin(2)  # 2개월 뒤\n",
    "        elif frequency == 'W':  # 주간\n",
    "            obs_df['adjusted_date'] = obs_df['date'] + pd.offsets.Week(1)  # 1주 뒤\n",
    "        else:\n",
    "            obs_df['adjusted_date'] = obs_df['date']  # 기타 주기\n",
    "\n",
    "        obs_df = obs_df[['adjusted_date', source_key]].set_index('adjusted_date')\n",
    "\n",
    "        rel_url = f'{base_url}/release/dates'\n",
    "        rel_params = {'api_key': api_key, 'file_type': 'json', 'release_id': release_id}\n",
    "        rel_response = requests.get(rel_url, params=rel_params, timeout=timeout)\n",
    "        rel_data = rel_response.json()['release_dates']\n",
    "        if not rel_data:\n",
    "            return None, None, \"No release dates\"\n",
    "\n",
    "        rel_df = pd.DataFrame(rel_data)\n",
    "        rel_df['release_date'] = pd.to_datetime(rel_df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "        rel_df = rel_df[['release_date']].sort_values('release_date')\n",
    "\n",
    "        temp_df = obs_df.reset_index()\n",
    "        temp_df = pd.merge_asof(temp_df, rel_df, left_on='adjusted_date', right_on='release_date', direction='forward')\n",
    "        temp_df = temp_df.drop(columns=['adjusted_date'])\n",
    "        temp_df = temp_df.groupby('release_date').first()\n",
    "        temp_df = temp_df.resample('D').asfreq()\n",
    "\n",
    "        return meta_df, temp_df, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None, str(e)\n",
    "\n",
    "def fetch_fred_data(keys, api_key, base_url, delay_seconds=0.3):\n",
    "    meta_dict = {}\n",
    "    data_dict = {}\n",
    "    skipped_keys = []\n",
    "\n",
    "    for source_key in keys:\n",
    "        print(f\"Processing {source_key}...\")\n",
    "        meta_df, series_df, error = fetch_fred_series(source_key, api_key, base_url)\n",
    "        if error:\n",
    "            print(f\"skip {source_key}: {error}\")\n",
    "            skipped_keys.append((source_key, error))\n",
    "        else:\n",
    "            meta_dict[source_key] = meta_df\n",
    "            data_dict[source_key] = series_df\n",
    "        time.sleep(delay_seconds)\n",
    "\n",
    "    return meta_dict, data_dict, skipped_keys\n",
    "\n",
    "# 실행\n",
    "market_df = fetch_yahoo_data(yf_tickers)\n",
    "\n",
    "mid_point = len(fred_keys) // 2\n",
    "first_half = fred_keys[:mid_point]\n",
    "second_half = fred_keys[mid_point:]\n",
    "\n",
    "print(\"Fetching first half of FRED keys...\")\n",
    "meta_dict1, data_dict1, skipped1 = fetch_fred_data(first_half, API_KEY, BASE_URL)\n",
    "\n",
    "print(\"Waiting before second half...\")\n",
    "time.sleep(60)\n",
    "\n",
    "print(\"Fetching second half of FRED keys...\")\n",
    "meta_dict2, data_dict2, skipped2 = fetch_fred_data(second_half, API_KEY, BASE_URL)\n",
    "\n",
    "meta_dict = {**meta_dict1, **meta_dict2}\n",
    "data_dict = {**data_dict1, **data_dict2}\n",
    "skipped_keys = skipped1 + skipped2\n",
    "\n",
    "fred_merged_df = pd.concat(data_dict.values(), axis=1, join='outer')\n",
    "fred_meta_full_df = pd.concat(meta_dict.values(), axis=0, ignore_index=True)\n",
    "final_df = pd.concat([market_df, fred_merged_df], axis=1)\n",
    "\n",
    "print(\"FRED 메타데이터 df shape:\", fred_meta_full_df.shape)\n",
    "print(\"FRED merged_df shape:\", fred_merged_df.shape)\n",
    "print(\"Market df shape:\", market_df.shape)\n",
    "print(\"Final df shape:\", final_df.shape)\n",
    "print(fred_meta_full_df.head())\n",
    "print(final_df.head())\n",
    "print(\"\\nSkipped keys:\")\n",
    "for key, reason in skipped_keys:\n",
    "    print(f\"{key}: {reason}\")\n",
    "\n",
    "# 주기별 데이터 확인\n",
    "print(\"\\nCPIAUCSL (월간):\")\n",
    "print(final_df[final_df['CPIAUCSL'].notna()][['CPIAUCSL']])\n",
    "print(\"\\nGASREGW (주간):\")\n",
    "print(final_df[final_df['GASREGW'].notna()][['GASREGW']])\n",
    "print(\"\\nGDPC1 (분기, 예시):\")\n",
    "print(final_df[final_df['GDPC1'].notna()][['GDPC1']] if 'GDPC1' in final_df.columns else \"GDPC1 not in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching first half of FRED keys...\n",
      "Processing CPIAUCSL...\n",
      "Processing CPILFESL...\n",
      "Processing CPIUFDSL...\n",
      "Processing CPIHOSSL...\n",
      "Processing CUSR0000SETB01...\n",
      "Waiting before second half...\n",
      "Fetching second half of FRED keys...\n",
      "Processing PCEPI...\n"
     ]
    }
   ],
   "source": [
    "# 필수 FRED series만 필터링\n",
    "fred_keys = [\n",
    "    'CPIAUCSL',  # CPI headline\n",
    "    'CPILFESL',  # Core CPI\n",
    "    'CPIUFDSL',  # CPI Food\n",
    "    'CPIHOSSL',  # CPI Food at home\n",
    "    'CUSR0000SETB01',  # CPI Gasoline\n",
    "    'PCEPI',  # PCE price index\n",
    "    'PCEPILFE',  # Core PCE price index\n",
    "    'DPCERD3A086NBEA',  # PCE food off-premises\n",
    "    'GASREGW',  # Weekly gasoline price\n",
    "    'DCOILWTICO'  # Crude oil (daily WTI 기준)\n",
    "]\n",
    "\n",
    "\n",
    "# 이후 fetch_fred_data() 부분 동일하게 실행\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = FRED_API_KEY\n",
    "BASE_URL = 'https://api.stlouisfed.org/fred'\n",
    "\n",
    "def fetch_fred_series(source_key, api_key, base_url, timeout=10):\n",
    "    try:\n",
    "        # 메타데이터\n",
    "        meta_url = f'{base_url}/series'\n",
    "        meta_params = {'api_key': api_key, 'file_type': 'json', 'series_id': source_key}\n",
    "        meta_response = requests.get(meta_url, params=meta_params, timeout=timeout)\n",
    "        meta_json = meta_response.json()\n",
    "        meta_r = meta_json.get('seriess', [])\n",
    "        if not meta_r:\n",
    "            return None, None, f\"No metadata available - Response: {meta_json}\"\n",
    "\n",
    "        meta_df = pd.DataFrame(meta_r)\n",
    "        frequency = meta_df['frequency_short'].iloc[0]  # 주기: D, W, M, Q 등\n",
    "\n",
    "        release_url = f'{base_url}/series/release'\n",
    "        release_params = {'api_key': api_key, 'file_type': 'json', 'series_id': source_key}\n",
    "        release_response = requests.get(release_url, params=release_params, timeout=timeout)\n",
    "        release_json = release_response.json()\n",
    "        if 'releases' not in release_json or not release_json['releases']:\n",
    "            return None, None, \"No release info\"\n",
    "        release_id = release_json['releases'][0]['id']\n",
    "\n",
    "        obs_url = f'{base_url}/series/observations'\n",
    "        obs_params = {'api_key': api_key, 'file_type': 'json', 'series_id': source_key}\n",
    "        obs_response = requests.get(obs_url, params=obs_params, timeout=timeout)\n",
    "        obs_json = obs_response.json()\n",
    "        if 'observations' not in obs_json or not obs_json['observations']:\n",
    "            return None, None, \"No observations or empty\"\n",
    "\n",
    "        obs_df = pd.DataFrame(obs_json['observations'])\n",
    "        obs_df['date'] = pd.to_datetime(obs_df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "        obs_df[source_key] = obs_df['value']\n",
    "\n",
    "        # 주기에 따라 날짜 조정\n",
    "        if frequency == 'M':  # 월간\n",
    "            obs_df['adjusted_date'] = obs_df['date'] + pd.offsets.MonthEnd(0) + pd.offsets.MonthBegin(1)\n",
    "        elif frequency == 'Q':  # 분기\n",
    "            obs_df['adjusted_date'] = obs_df['date'] + pd.offsets.QuarterEnd(0) + pd.offsets.MonthBegin(2)  # 2개월 뒤\n",
    "        elif frequency == 'W':  # 주간\n",
    "            obs_df['adjusted_date'] = obs_df['date'] + pd.offsets.Week(1)  # 1주 뒤\n",
    "        else:\n",
    "            obs_df['adjusted_date'] = obs_df['date']  # 기타 주기\n",
    "\n",
    "        obs_df = obs_df[['adjusted_date', source_key]].set_index('adjusted_date')\n",
    "\n",
    "        rel_url = f'{base_url}/release/dates'\n",
    "        rel_params = {'api_key': api_key, 'file_type': 'json', 'release_id': release_id}\n",
    "        rel_response = requests.get(rel_url, params=rel_params, timeout=timeout)\n",
    "        rel_data = rel_response.json()['release_dates']\n",
    "        if not rel_data:\n",
    "            return None, None, \"No release dates\"\n",
    "\n",
    "        rel_df = pd.DataFrame(rel_data)\n",
    "        rel_df['release_date'] = pd.to_datetime(rel_df['date'], format='%Y-%m-%d', errors='coerce')\n",
    "        rel_df = rel_df[['release_date']].sort_values('release_date')\n",
    "\n",
    "        temp_df = obs_df.reset_index()\n",
    "        temp_df = pd.merge_asof(temp_df, rel_df, left_on='adjusted_date', right_on='release_date', direction='forward')\n",
    "        temp_df = temp_df.drop(columns=['adjusted_date'])\n",
    "        temp_df = temp_df.groupby('release_date').first()\n",
    "        temp_df = temp_df.resample('D').asfreq()\n",
    "\n",
    "        return meta_df, temp_df, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None, str(e)\n",
    "\n",
    "def fetch_fred_data(keys, api_key, base_url, delay_seconds=0.1):\n",
    "    meta_dict = {}\n",
    "    data_dict = {}\n",
    "    skipped_keys = []\n",
    "\n",
    "    for source_key in keys:\n",
    "        print(f\"Processing {source_key}...\")\n",
    "        meta_df, series_df, error = fetch_fred_series(source_key, api_key, base_url)\n",
    "        if error:\n",
    "            print(f\"skip {source_key}: {error}\")\n",
    "            skipped_keys.append((source_key, error))\n",
    "        else:\n",
    "            meta_dict[source_key] = meta_df\n",
    "            data_dict[source_key] = series_df\n",
    "        time.sleep(delay_seconds)\n",
    "\n",
    "    return meta_dict, data_dict, skipped_keys\n",
    "\n",
    "# 실행\n",
    "\n",
    "mid_point = len(fred_keys) // 2\n",
    "first_half = fred_keys[:mid_point]\n",
    "second_half = fred_keys[mid_point:]\n",
    "\n",
    "print(\"Fetching first half of FRED keys...\")\n",
    "meta_dict1, data_dict1, skipped1 = fetch_fred_data(first_half, API_KEY, BASE_URL)\n",
    "\n",
    "print(\"Waiting before second half...\")\n",
    "\n",
    "print(\"Fetching second half of FRED keys...\")\n",
    "meta_dict2, data_dict2, skipped2 = fetch_fred_data(second_half, API_KEY, BASE_URL)\n",
    "\n",
    "meta_dict = {**meta_dict1, **meta_dict2}\n",
    "data_dict = {**data_dict1, **data_dict2}\n",
    "skipped_keys = skipped1 + skipped2\n",
    "\n",
    "fred_merged_df = pd.concat(data_dict.values(), axis=1, join='outer')\n",
    "fred_meta_full_df = pd.concat(meta_dict.values(), axis=0, ignore_index=True)\n",
    "\n",
    "print(\"FRED 메타데이터 df shape:\", fred_meta_full_df.shape)\n",
    "print(\"FRED merged_df shape:\", fred_merged_df.shape)\n",
    "print(fred_meta_full_df.head())\n",
    "print(\"\\nSkipped keys:\")\n",
    "for key, reason in skipped_keys:\n",
    "    print(f\"{key}: {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../data/final_df.csv')\n",
    "fred_meta_full_df.to_csv('../data/fred_meta_full_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 거시경제 지표의 메타 정보를 인기 순으로 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id                                              title  \\\n",
      "0                T10Y2Y  10-Year Treasury Constant Maturity Minus 2-Yea...   \n",
      "1              FEDFUNDS                       Federal Funds Effective Rate   \n",
      "2              CPIAUCSL  Consumer Price Index for All Urban Consumers: ...   \n",
      "3                UNRATE                                  Unemployment Rate   \n",
      "4                T10Y3M  10-Year Treasury Constant Maturity Minus 3-Mon...   \n",
      "...                 ...                                                ...   \n",
      "5275  BOGZ1FL263061145Q  Rest of the World; Treasury Securities Held by...   \n",
      "5276  BOGZ1LM503062003Q  Other Financial Business; Municipal Securities...   \n",
      "5277           FFWSJLOW  Low Value of the Federal Funds Rate for the In...   \n",
      "5278            FFHTLOW  Low Value of the Federal Funds Rate for the In...   \n",
      "5279             WUIBRA                 World Uncertainty Index for Brazil   \n",
      "\n",
      "      popularity  \n",
      "0            100  \n",
      "1             98  \n",
      "2             95  \n",
      "3             95  \n",
      "4             94  \n",
      "...          ...  \n",
      "5275          10  \n",
      "5276          10  \n",
      "5277          10  \n",
      "5278          10  \n",
      "5279          10  \n",
      "\n",
      "[5280 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 모든 소스 가져오기\n",
    "source_url = f'{BASE_URL}/sources'\n",
    "source_params = {'api_key': API_KEY, 'file_type': 'json'}\n",
    "sources = requests.get(source_url, params=source_params).json()['sources']\n",
    "source_ids = [s['id'] for s in sources]\n",
    "\n",
    "all_meta = []\n",
    "\n",
    "# Step 2: 소스별 시리즈 수집\n",
    "for sid in source_ids:\n",
    "    series_url = f'{BASE_URL}/source/releases'\n",
    "    series_params = {\n",
    "        'api_key': API_KEY,\n",
    "        'file_type': 'json',\n",
    "        'source_id': sid\n",
    "    }\n",
    "    releases = requests.get(series_url, params=series_params).json().get('releases', [])\n",
    "    for release in releases:\n",
    "        rel_id = release['id']\n",
    "        series_fetch_url = f'{BASE_URL}/release/series'\n",
    "        series_fetch_params = {\n",
    "            'api_key': API_KEY,\n",
    "            'file_type': 'json',\n",
    "            'release_id': rel_id,\n",
    "            'order_by': 'popularity',\n",
    "            'sort_order': 'desc',\n",
    "            'limit': 1000\n",
    "        }\n",
    "        series_data = requests.get(series_fetch_url, params=series_fetch_params).json()\n",
    "        all_meta.extend(series_data.get('seriess', []))\n",
    "\n",
    "# Step 3: DataFrame 변환 및 필터링\n",
    "meta_df = pd.DataFrame(all_meta)\n",
    "meta_df = meta_df.drop_duplicates(subset=['id'])\n",
    "meta_df = meta_df[meta_df['popularity'] >= 10].sort_values('popularity', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(meta_df[['id', 'title', 'popularity']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
